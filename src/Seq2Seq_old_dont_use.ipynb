{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from read_data import read_data\n",
    "import numpy as np\n",
    "\n",
    "def compute_delta_learning_metrics(data):\n",
    "    \"\"\"\n",
    "    Computes additional metrics for the dataset:\n",
    "    - Delta Position: Leader's position minus Follower's position.\n",
    "    - Delta Velocity: Leader's velocity minus Follower's velocity.\n",
    "    - Delta Acceleration: Leader's acceleration minus Follower's acceleration.\n",
    "    - Time-To-Collision (TTC): Delta Position divided by Delta Velocity.\n",
    "    \"\"\"\n",
    "    data[\"delta_position\"] = data[\"x_leader\"] - data[\"x_follower\"]\n",
    "    data[\"delta_velocity\"] = data[\"v_follower\"] - data[\"v_leader\"]\n",
    "\n",
    "    return data\n",
    "\n",
    "def compute_delta_metrics(data):\n",
    "    \"\"\"\n",
    "    Computes additional metrics for the dataset:\n",
    "    - Delta Position: Leader's position minus Follower's position.\n",
    "    - Delta Velocity: Leader's velocity minus Follower's velocity.\n",
    "    - Delta Acceleration: Leader's acceleration minus Follower's acceleration.\n",
    "    - Time-To-Collision (TTC): Delta Position divided by Delta Velocity.\n",
    "    \"\"\"\n",
    "    data[\"delta_position\"] = data[\"x_leader\"] - data[\"x_follower\"]\n",
    "    data[\"delta_velocity\"] = data[\"v_follower\"] - data[\"v_leader\"]\n",
    "    data[\"delta_acceleration\"] = data[\"a_follower\"] - data[\"a_leader\"]\n",
    "    data[\"TTC\"] = data[\"delta_position\"] / data[\"delta_velocity\"]\n",
    "    data.loc[data[\"TTC\"] < 0, \"TTC\"] = np.nan\n",
    "    data['time_headway'] = data['delta_position'] / data['v_follower']\n",
    "    data['TTC_min'] = data['TTC']\n",
    "\n",
    "    # Calculate jerk for the follower vehicle\n",
    "    data['jerk_follower'] = np.gradient(data['a_follower'], data['time'])\n",
    "    return data\n",
    "\n",
    "def aggregate_data_by_case(data):\n",
    "    \"\"\"\n",
    "    Aggregates the dataset by 'case_id' to find the max and min \n",
    "    of each delta metric and TTC for each case.\n",
    "    Renames columns for clarity and adds case_id as a column.\n",
    "    \"\"\"\n",
    "    aggr_data = data.groupby('case_id').agg({'delta_velocity':'mean',\n",
    "                           'v_follower':'max',\n",
    "                           'delta_acceleration':'mean',\n",
    "                           'a_follower':'max',\n",
    "                           'jerk_follower':'mean',\n",
    "                           'time_headway':'median',\n",
    "                           'delta_position':'min',\n",
    "                           'TTC':'median',\n",
    "                           'TTC_min':'min'}).reset_index()\n",
    "\n",
    "    return aggr_data\n",
    "\n",
    "def adjust_ttc_sign(aggregated_data):\n",
    "    \"\"\"\n",
    "    Ensures TTC (Time-To-Collision) is non-negative by taking the absolute value.\n",
    "    \"\"\"\n",
    "    aggregated_data[\"TTC\"] = aggregated_data[\"TTC\"].abs()\n",
    "    aggregated_data[\"TTC_min\"] = aggregated_data[\"TTC_min\"].abs()\n",
    "    return aggregated_data\n",
    "\n",
    "def convert_df(dataset: str, mode: str):\n",
    "    \"\"\"\n",
    "    Main function that utilizes the above helper functions to preprocess the data.\n",
    "    Returns a DataFrame grouped by 'case_id' with max and min values of\n",
    "    delta position, delta velocity, delta acceleration, and TTC (Time-To-Collision).\n",
    "    \"\"\"\n",
    "    data = read_data(dataset, mode)\n",
    "    data = compute_delta_metrics(data)\n",
    "    aggregated_data = aggregate_data_by_case(data)\n",
    "    aggregated_data = adjust_ttc_sign(aggregated_data)\n",
    "    return aggregated_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainHA = convert_df(\"HA\", \"train\")\n",
    "# trainHA.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Constants for settings that may change\n",
    "RANDOM_STATE = 42\n",
    "PCA_COMPONENTS = 2\n",
    "\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Normalize the features of a dataframe.\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    features_numeric = features.select_dtypes(include=np.number).dropna()\n",
    "    normalized_data = scaler.fit_transform(features_numeric)\n",
    "    return normalized_data, features_numeric\n",
    "\n",
    "\n",
    "def apply_dimensionality_reduction(data, n_components=PCA_COMPONENTS):\n",
    "    \"\"\"Apply PCA to reduce dimensions of the data.\"\"\"\n",
    "    pca = PCA(n_components=n_components)\n",
    "    return pca.fit_transform(data)\n",
    "\n",
    "\n",
    "def perform_clustering(data, n_clusters):\n",
    "    \"\"\"Perform KMeans clustering on the data.\"\"\"\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=RANDOM_STATE)\n",
    "    labels = kmeans.fit_predict(data)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def plot_clusters(features, labels, pca_data=None):\n",
    "    \"\"\"Plot the results of clustering.\"\"\"\n",
    "    if pca_data is not None:\n",
    "        plt.scatter(pca_data[:, 0], pca_data[:, 1], c=labels, cmap=\"viridis\")\n",
    "        plt.xlabel(\"PCA Component 1\")\n",
    "        plt.ylabel(\"PCA Component 2\")\n",
    "        plt.title(\"K-Means Clustering with PCA\")\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(15, 7))\n",
    "        ax = fig.add_subplot(121, projection=\"3d\")\n",
    "        ax.scatter(\n",
    "            features.iloc[:, 0],\n",
    "            features.iloc[:, 1],\n",
    "            features.iloc[:, 2],\n",
    "            c=labels,\n",
    "            cmap=\"viridis\",\n",
    "            s=50,\n",
    "        )\n",
    "        ax.set_xlabel(\"Feature 1\")\n",
    "        ax.set_ylabel(\"Feature 2\")\n",
    "        ax.set_zlabel(\"Feature 3\")\n",
    "        ax.set_title(\"K-Means Clustering Results\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def find_optimal_clusters(data, max_clusters=5, silent = True):\n",
    "    \"\"\"Determine the optimal cluster count using silhouette score and elbow method.\"\"\"\n",
    "    inertia_list = []\n",
    "    silhouette_scores = []\n",
    "    for n_clusters in range(2, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=RANDOM_STATE)\n",
    "        labels = kmeans.fit_predict(data)\n",
    "        inertia_list.append(kmeans.inertia_)\n",
    "        silhouette_scores.append(silhouette_score(data, labels))\n",
    "\n",
    "    if silent:\n",
    "    # Elbow Method Plot\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(range(2, max_clusters + 1), inertia_list, \"o-\")\n",
    "        plt.xlabel(\"Number of Clusters\")\n",
    "        plt.ylabel(\"Inertia\")\n",
    "        plt.title(\"Elbow Method\")\n",
    "\n",
    "        # Silhouette Score Plot\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(range(2, max_clusters + 1), silhouette_scores, \"o-\")\n",
    "        plt.xlabel(\"Number of Clusters\")\n",
    "        plt.ylabel(\"Silhouette Score\")\n",
    "        plt.title(\"Silhouette Scores for Various Clusters\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Assuming the elbow is at the cluster number with the highest silhouette score\n",
    "    optimal_clusters = (\n",
    "        np.argmax(silhouette_scores) + 2\n",
    "    )  # +2 because range starts from 2\n",
    "    return optimal_clusters\n",
    "\n",
    "\n",
    "def get_clustered_df(features, silent = True):\n",
    "    \"\"\"Main function to execute the clustering analysis pipeline.\"\"\"\n",
    "    # Load and preprocess the data\n",
    "\n",
    "    normalized_data, features_numeric = preprocess_features(features)\n",
    "\n",
    "    # Optionally apply PCA\n",
    "    pca_data = apply_dimensionality_reduction(normalized_data)\n",
    "\n",
    "    # Find the optimal number of clusters\n",
    "    optimal_clusters = find_optimal_clusters(pca_data)\n",
    "\n",
    "    # Perform clustering with the optimal number of clusters\n",
    "    labels = perform_clustering(pca_data, optimal_clusters)\n",
    "    features_numeric[\"cluster\"] = labels\n",
    "\n",
    "    # Plot the results\n",
    "    if not silent:\n",
    "        plot_clusters(features_numeric, labels, pca_data)\n",
    "\n",
    "    # Compute and display the average silhouette score\n",
    "    silhouette_avg = silhouette_score(pca_data, labels)\n",
    "    print(f\"The average silhouette_score is: {silhouette_avg}\")\n",
    "\n",
    "    return features_numeric\n",
    "\n",
    "# clustered_data = get_clustered_df(trainHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_df(dataset: str, clustered_data: pd.DataFrame, mode: str):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with delta position, delta velocity, v_follower and cluster.\n",
    "    \"\"\"\n",
    "    data = read_data(dataset, mode)\n",
    "    data[\"delta_position\"] = data[\"x_leader\"] - data[\"x_follower\"]\n",
    "    data[\"delta_velocity\"] = data[\"v_follower\"] - data[\"v_leader\"]\n",
    "    \n",
    "    # Merge the data with clustered_data on 'case_id' to get the 'cluster' column\n",
    "    data = pd.merge(data, clustered_data[['case_id', 'cluster']], on='case_id', how='left')\n",
    "    data = data[[\"delta_position\", \"delta_velocity\", \"v_follower\", \"cluster\"]]\n",
    "    # print(data.head())\n",
    "    return data\n",
    "\n",
    "\n",
    "# train_data = train_data(\"HA\", clustered_data, \"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs, (hx, cx) = self.lstm(x)\n",
    "        return hx[-1], cx[-1]\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        outputs, (hidden, cell) = self.lstm(x, (hidden.unsqueeze(0), cell.unsqueeze(0)))\n",
    "        predictions = self.linear(outputs)\n",
    "        return predictions, hidden[-1], cell[-1]\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_steps_out):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder(input_size, hidden_size)\n",
    "        self.decoder = Decoder(hidden_size, output_size)\n",
    "        self.n_steps_out = n_steps_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden, cell = self.encoder(x)\n",
    "        # 初始化解码器的输入\n",
    "        decoder_input = torch.zeros((x.size(0), self.n_steps_out, self.decoder.lstm.input_size)).to(x.device)\n",
    "        outputs, _, _ = self.decoder(decoder_input, hidden, cell)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def preprocess_data(df, n_steps_in=30, n_steps_out=10, test_size=0.2):\n",
    "    # 只保留需要的列\n",
    "    df = df[[\"delta_position\", \"delta_velocity\", \"v_follower\"]]\n",
    "    scaler = StandardScaler()\n",
    "    data_normalized = scaler.fit_transform(df)\n",
    "    X, y = create_sequences(data_normalized, n_steps_in, n_steps_out)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    return (\n",
    "        torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32),\n",
    "        torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32),\n",
    "        scaler\n",
    "    )\n",
    "\n",
    "\n",
    "def create_sequences(data, n_steps_in, n_steps_out):\n",
    "    X, y = [], []\n",
    "    for i in range(0, len(data) - n_steps_in - n_steps_out + 5):\n",
    "        seq_x = data[i:i + n_steps_in]\n",
    "        seq_y = data[i + n_steps_in:i + n_steps_in + n_steps_out]\n",
    "        if seq_x.shape[0] == n_steps_in and seq_y.shape[0] == n_steps_out:\n",
    "            X.append(seq_x)\n",
    "            y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "# def train_model(model, X_train_tensor, y_train_tensor, epochs, optimizer, loss_function):\n",
    "#     for epoch in tqdm(range(epochs), desc='Training Epochs'):\n",
    "#         model.train()\n",
    "#         optimizer.zero_grad()\n",
    "#         y_pred = model(X_train_tensor)\n",
    "#         loss = loss_function(y_pred, y_train_tensor)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         if epoch % 10 == 0:\n",
    "#             tqdm.write(f'Epoch: {epoch} Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test_tensor, y_test_tensor, scaler, device, batch_size=1024):\n",
    "    # Move the model to the specified device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Create a DataLoader for the test data\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    y_pred_list = []\n",
    "    y_test_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            # Move the tensors to the same device as the model\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            # Predict\n",
    "            y_pred = model(X_batch)\n",
    "            # Move the predictions back to CPU\n",
    "            y_pred_list.append(y_pred.cpu())\n",
    "            y_test_list.append(y_batch.cpu())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    y_pred_numpy = torch.cat(y_pred_list).numpy()\n",
    "    y_test_numpy = torch.cat(y_test_list).numpy()\n",
    "    \n",
    "    # Inverse transform the predictions and true values\n",
    "    y_pred_original = scaler.inverse_transform(y_pred_numpy.reshape(-1, y_pred_numpy.shape[-1])).reshape(y_pred_numpy.shape)\n",
    "    y_test_original = scaler.inverse_transform(y_test_numpy.reshape(-1, y_test_numpy.shape[-1])).reshape(y_test_numpy.shape)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test_original[:, 0, :], y_pred_original[:, 0, :])\n",
    "    rmse = math.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test_original[:, 0, :], y_pred_original[:, 0, :])\n",
    "    \n",
    "    print(f'MSE: {mse:.2f}, RMSE: {rmse:.2f}, MAE: {mae:.2f}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, dataloader, epochs, optimizer, loss_function, device=\"cuda\", accumulation_steps=4):\n",
    "    model.to(device)  # Ensure model is on the correct device\n",
    "\n",
    "    for epoch in tqdm(range(epochs), position=2, leave=False, desc=\"j\", colour='red'):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()  # Reset gradients tensors\n",
    "        for i, (X_batch, y_batch) in enumerate(dataloader):\n",
    "            print(f\"Batch {i} - X_batch shape: {X_batch.shape}, y_batch shape: {y_batch.shape}\")  # Debugging line\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # Move batch data to the device\n",
    "\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_function(y_pred, y_batch) / accumulation_steps  # Normalize our loss\n",
    "\n",
    "            loss.backward()\n",
    "            if (i + 1) % accumulation_steps == 0 or i + 1 == len(dataloader):\n",
    "                optimizer.step()  # Perform a single optimization step\n",
    "                optimizer.zero_grad()  # Reset gradients tensors\n",
    "\n",
    "            # Clear some memory\n",
    "            del X_batch, y_batch, y_pred\n",
    "            gc.collect()  # Force garbage collection\n",
    "            if device == 'cuda':\n",
    "                torch.cuda.empty_cache()  # Clear cache if on GPU\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            tqdm.write(f'Epoch: {epoch} Loss: {loss.item() * accumulation_steps:.4f}')  # Adjust the loss value\n",
    "\n",
    "\n",
    "\n",
    "def run_training(train_data,dataset, n_steps_in=30, n_steps_out=10, epochs=1, lr=0.01, device=\"cuda\" ):\n",
    "    clustered_dataframes = {}\n",
    "    for cluster_number in train_data[\"cluster\"].unique():\n",
    "        clustered_dataframes[cluster_number] = train_data[train_data[\"cluster\"] == cluster_number]\n",
    "    print(\"cluster is over\")\n",
    "    models_scalers = {}\n",
    "\n",
    "    for cluster, cluster_df in tqdm(clustered_dataframes.items(), position=1, leave=False, desc=\"i\", colour='green'):\n",
    "        # for j in tqdm(range(10), desc=\"j\", colour='red'):\n",
    "            # time.sleep(0.5)\n",
    "    # for cluster, cluster_df in clustered_dataframes.items():\n",
    "        if cluster_df.empty:\n",
    "            print(f\"Cluster {cluster} is empty.\")\n",
    "            continue\n",
    "\n",
    "        (\n",
    "            X_train_tensor,\n",
    "            y_train_tensor,\n",
    "            X_test_tensor,\n",
    "            y_test_tensor,\n",
    "            scaler,\n",
    "        ) = preprocess_data(cluster_df, n_steps_in, n_steps_out)\n",
    "\n",
    "        # X_train_tensor = X_train_tensor.to(device)\n",
    "        # y_train_tensor = y_train_tensor.to(device)\n",
    "        # X_test_tensor = X_test_tensor.to(device)\n",
    "        # y_test_tensor = y_test_tensor.to(device)\n",
    "        # Create a DataLoader for batching\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        # Use num_workers and pin_memory for faster data loading\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=256, \n",
    "            shuffle=True, \n",
    "            num_workers=10,  # or more, depending on your CPU and data\n",
    "            pin_memory=True  # helps with faster data transfer to GPU\n",
    "        )        \n",
    "\n",
    "        model = Seq2Seq(\n",
    "            input_size=X_train_tensor.shape[2],\n",
    "            hidden_size=50,\n",
    "            output_size=y_train_tensor.shape[2],\n",
    "            n_steps_out=n_steps_out,\n",
    "        )\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        loss_function = nn.MSELoss()\n",
    "\n",
    "        model.to(device)  # Move model to the device (GPU or CPU)\n",
    "        \n",
    "        train_model(\n",
    "            model,\n",
    "            train_dataloader,  # Pass the DataLoader instead of tensors directly\n",
    "            epochs,\n",
    "            optimizer,\n",
    "            loss_function,\n",
    "            device  # Pass the device to the training function\n",
    "        )\n",
    "\n",
    "        print(\"Train is over\")\n",
    "\n",
    "        # Save the model and scaler for this cluster\n",
    "        torch.save(\n",
    "            {\"model_state_dict\": model.state_dict(), \"scaler\": scaler},\n",
    "            f\"{dataset}model_scaler_cluster_{cluster}.pth\",\n",
    "        )\n",
    "        \n",
    "        evaluate_model(model, X_test_tensor, y_test_tensor, scaler,\"cuda\")\n",
    "\n",
    "        \n",
    "        models_scalers[cluster] = (model, scaler)\n",
    "\n",
    "    return models_scalers\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `train_data` is your DataFrame containing the training data.\n",
    "# models_scalers = run_training(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum number of input data is 35. And we take 40 as example, it will create 40 - n_steps_input(30) - n_steps_output(10) + 5 = 5 sequences. And each sequence will create a prediction, so a 40*3 data input will generate 5*30*3 sequences and resulting 5*10*3 predictions. And here I select the first predict as the output, for reasoning I write in the comment of the code.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation dataset is used here to predict the next time step\n",
    "# eval_df = evaluate_df(\"HA\", \"val\")\n",
    "# eval_df.head(10)\n",
    "# # print(eval_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from read_data import read_data\n",
    "\n",
    "def evaluate_df(dataset: str, mode: str):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with delta position, delta velocity, v_follower and cluster.\n",
    "    \"\"\"\n",
    "    data = read_data(dataset, mode)\n",
    "    data = compute_delta_metrics(data)\n",
    "    # only keep the columns of delta position, delta velocity, v_follower\n",
    "    data = data[['delta_position', 'delta_velocity', 'v_follower']]\n",
    "    \n",
    "    \n",
    "    print(data.head())\n",
    "    return data\n",
    "\n",
    "# Define the Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        return hidden, cell\n",
    "\n",
    "# Define the Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        outputs, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "        predictions = self.linear(outputs)\n",
    "        return predictions, hidden, cell\n",
    "\n",
    "# Define the Seq2Seq model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_steps_out):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder(input_size, hidden_size)\n",
    "        self.decoder = Decoder(hidden_size, output_size)\n",
    "        self.n_steps_out = n_steps_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden, cell = self.encoder(x)\n",
    "        decoder_input = torch.zeros((x.size(0), self.n_steps_out, self.decoder.lstm.input_size)).to(x.device)\n",
    "        outputs, _, _ = self.decoder(decoder_input, hidden, cell)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data_for_inference(df, n_steps_in=30, n_steps_out=10):\n",
    "    # Normalize the data\n",
    "    data_normalized = scaler.transform(df)  # Note: use transform, not fit_transform\n",
    "    # print(data_normalized.shape)\n",
    "    X = create_sequences_for_inference(data_normalized, n_steps_in, n_steps_out)\n",
    "    # print(X.shape)\n",
    "    return torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "def create_sequences_for_inference(data, n_steps_in, n_steps_out):\n",
    "    X = []\n",
    "    for i in range(0, len(data) - n_steps_in - n_steps_out + 5):\n",
    "        seq_x = data[i:i + n_steps_in]\n",
    "        if seq_x.shape[0] == n_steps_in:\n",
    "            X.append(seq_x)\n",
    "    return np.array(X)\n",
    "\n",
    "# need a simpler function here\n",
    "def process_runtime_data(df):\n",
    "    data_normalized = scaler.transform(df)\n",
    "    X = np.array(data_normalized)\n",
    "    return torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "def predict_delta_velocity(eval_df,mode_name, n_steps_in=30, delta_velocity_index=1):\n",
    "    \"\"\"\n",
    "    Safety-Critical Applications: If the prediction is used for real-time safety systems \n",
    "    (like advanced driver-assistance systems, ADAS), the most recent predictions may be the most\n",
    "     valuable as they can inform immediate safety interventions.\n",
    "\n",
    "    Driver Profiling or Long-Term Trends: If the goal is to understand long-term driver behavior\n",
    "     for insurance purposes or driver coaching, then averaging or aggregating over a range of predictions\n",
    "      to get a more stable and generalized profile might be more appropriate.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # Preprocess the data\n",
    "    X_new_tensor  = preprocess_data_for_inference(eval_df)\n",
    "    print(f\"X_new_tensor\",X_new_tensor.shape)\n",
    "    # Initialize the model based on the shape of the input data\n",
    "    model = Seq2Seq(input_size=X_new_tensor.shape[2], hidden_size=128, n_steps_out=10, output_size=X_new_tensor.shape[2])\n",
    "    \n",
    "    \n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(mode_name)\n",
    "\n",
    "    # Extract the scaler from the checkpoint\n",
    "    scaler = checkpoint['scaler']\n",
    "    # Load model's state dictionary from the checkpoint\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # Predict using the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_new_pred_tensor = model(X_new_tensor)\n",
    "        y_new_pred = y_new_pred_tensor.numpy()\n",
    "    # print(f\"y_new_pred_tensor\",y_test_tensor.shape)\n",
    "    print(y_new_pred.shape)\n",
    "\n",
    "    # Take the first prediction from the first sequence\n",
    "    y_first_pred = y_new_pred[0, :, :]\n",
    "    # print(f\"first predict\", y_first_pred.shape)\n",
    "    # print(y_first_pred)\n",
    "    # Inverse transform the predictions to the original scale\n",
    "\n",
    "    y_first_pred_original = scaler.inverse_transform(y_first_pred)\n",
    "\n",
    "    # Extract the denormalized delta_velocity values\n",
    "    delta_velocity_pred_original = y_first_pred_original[:, delta_velocity_index]\n",
    "    \n",
    "    # Return the predicted delta velocity\n",
    "    return delta_velocity_pred_original\n",
    "\n",
    "# Sample usage\n",
    "# delta_velocity_pred = predict_delta_velocity(eval_df)\n",
    "# print(f\"Predicted result:\", delta_velocity_pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a sequence of 10 delta_velocity, so we may use the average number here for simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                        \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/var/www/car-following-project/src/Seq2Seq.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/var/www/car-following-project/src/Seq2Seq.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m eval_mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/var/www/car-following-project/src/Seq2Seq.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m trainHA \u001b[39m=\u001b[39m convert_df(dataset, train_mode)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/var/www/car-following-project/src/Seq2Seq.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m clustered_data \u001b[39m=\u001b[39m get_clustered_df(trainHA)\n\u001b[1;32m     <a href='vscode-notebook-cell:/var/www/car-following-project/src/Seq2Seq.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m train_data \u001b[39m=\u001b[39m train_df(dataset, clustered_data, train_mode)\n\u001b[1;32m     <a href='vscode-notebook-cell:/var/www/car-following-project/src/Seq2Seq.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# print(train_data.describe())\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/var/www/car-following-project/src/Seq2Seq.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# train_data = train_data.sample(frac=0.01, random_state=1)\u001b[39;00m\n",
      "\u001b[1;32m/var/www/car-following-project/src/Seq2Seq.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/var/www/car-following-project/src/Seq2Seq.ipynb#X13sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m pca_data \u001b[39m=\u001b[39m apply_dimensionality_reduction(normalized_data)\n\u001b[1;32m    <a href='vscode-notebook-cell:/var/www/car-following-project/src/Seq2Seq.ipynb#X13sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m \u001b[39m# Find the optimal number of clusters\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/var/www/car-following-project/src/Seq2Seq.ipynb#X13sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m optimal_clusters \u001b[39m=\u001b[39m find_optimal_clusters(pca_data)\n\u001b[1;32m    <a href='vscode-notebook-cell:/var/www/car-following-project/src/Seq2Seq.ipynb#X13sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m \u001b[39m# Perform clustering with the optimal number of clusters\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/var/www/car-following-project/src/Seq2Seq.ipynb#X13sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m labels \u001b[39m=\u001b[39m perform_clustering(pca_data, optimal_clusters)\n",
      "\u001b[1;32m/var/www/car-following-project/src/Seq2Seq.ipynb Cell 11\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/var/www/car-following-project/src/Seq2Seq.ipynb#X13sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     labels \u001b[39m=\u001b[39m kmeans\u001b[39m.\u001b[39mfit_predict(data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/var/www/car-following-project/src/Seq2Seq.ipynb#X13sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     inertia_list\u001b[39m.\u001b[39mappend(kmeans\u001b[39m.\u001b[39minertia_)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/var/www/car-following-project/src/Seq2Seq.ipynb#X13sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     silhouette_scores\u001b[39m.\u001b[39mappend(silhouette_score(data, labels))\n\u001b[1;32m     <a href='vscode-notebook-cell:/var/www/car-following-project/src/Seq2Seq.ipynb#X13sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39mif\u001b[39;00m silent:\n\u001b[1;32m     <a href='vscode-notebook-cell:/var/www/car-following-project/src/Seq2Seq.ipynb#X13sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39m# Elbow Method Plot\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/var/www/car-following-project/src/Seq2Seq.ipynb#X13sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m5\u001b[39m))\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/car-following-project-UTHu64Qq/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    215\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/car-following-project-UTHu64Qq/lib/python3.11/site-packages/sklearn/metrics/cluster/_unsupervised.py:130\u001b[0m, in \u001b[0;36msilhouette_score\u001b[0;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         X, labels \u001b[39m=\u001b[39m X[indices], labels[indices]\n\u001b[0;32m--> 130\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(silhouette_samples(X, labels, metric\u001b[39m=\u001b[39;49mmetric, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds))\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/car-following-project-UTHu64Qq/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:187\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m global_skip_validation \u001b[39m=\u001b[39m get_config()[\u001b[39m\"\u001b[39m\u001b[39mskip_parameter_validation\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    186\u001b[0m \u001b[39mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 187\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    189\u001b[0m func_sig \u001b[39m=\u001b[39m signature(func)\n\u001b[1;32m    191\u001b[0m \u001b[39m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/car-following-project-UTHu64Qq/lib/python3.11/site-packages/sklearn/metrics/cluster/_unsupervised.py:282\u001b[0m, in \u001b[0;36msilhouette_samples\u001b[0;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[1;32m    278\u001b[0m kwds[\u001b[39m\"\u001b[39m\u001b[39mmetric\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m metric\n\u001b[1;32m    279\u001b[0m reduce_func \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(\n\u001b[1;32m    280\u001b[0m     _silhouette_reduce, labels\u001b[39m=\u001b[39mlabels, label_freqs\u001b[39m=\u001b[39mlabel_freqs\n\u001b[1;32m    281\u001b[0m )\n\u001b[0;32m--> 282\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39;49m(\u001b[39m*\u001b[39;49mpairwise_distances_chunked(X, reduce_func\u001b[39m=\u001b[39;49mreduce_func, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds))\n\u001b[1;32m    283\u001b[0m intra_clust_dists, inter_clust_dists \u001b[39m=\u001b[39m results\n\u001b[1;32m    284\u001b[0m intra_clust_dists \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(intra_clust_dists)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/car-following-project-UTHu64Qq/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:2027\u001b[0m, in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   2025\u001b[0m \u001b[39mif\u001b[39;00m reduce_func \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2026\u001b[0m     chunk_size \u001b[39m=\u001b[39m D_chunk\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m-> 2027\u001b[0m     D_chunk \u001b[39m=\u001b[39m reduce_func(D_chunk, sl\u001b[39m.\u001b[39;49mstart)\n\u001b[1;32m   2028\u001b[0m     _check_chunk_size(D_chunk, chunk_size)\n\u001b[1;32m   2029\u001b[0m \u001b[39myield\u001b[39;00m D_chunk\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/car-following-project-UTHu64Qq/lib/python3.11/site-packages/sklearn/metrics/cluster/_unsupervised.py:171\u001b[0m, in \u001b[0;36m_silhouette_reduce\u001b[0;34m(D_chunk, start, labels, label_freqs)\u001b[0m\n\u001b[1;32m    169\u001b[0m         sample_weights \u001b[39m=\u001b[39m D_chunk[i]\n\u001b[1;32m    170\u001b[0m         sample_labels \u001b[39m=\u001b[39m labels\n\u001b[0;32m--> 171\u001b[0m         cluster_distances[i] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mbincount(\n\u001b[1;32m    172\u001b[0m             sample_labels, weights\u001b[39m=\u001b[39;49msample_weights, minlength\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(label_freqs)\n\u001b[1;32m    173\u001b[0m         )\n\u001b[1;32m    175\u001b[0m \u001b[39m# intra_index selects intra-cluster distances within cluster_distances\u001b[39;00m\n\u001b[1;32m    176\u001b[0m end \u001b[39m=\u001b[39m start \u001b[39m+\u001b[39m n_chunk_samples\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set the dataset and mode\n",
    "if __name__ == \"__main__\":\n",
    "    datasets = [\"HA\",\"HH\"]\n",
    "    for i in tqdm(datasets, position=0, leave=False, desc=\"i\", colour='green'):\n",
    "        dataset = i\n",
    "        train_mode = \"train\"\n",
    "        eval_mode = \"val\"\n",
    "\n",
    "\n",
    "        trainHA = convert_df(dataset, train_mode)\n",
    "        clustered_data = get_clustered_df(trainHA)\n",
    "        train_data = train_df(dataset, clustered_data, train_mode)\n",
    "        # print(train_data.describe())\n",
    "        # train_data = train_data.sample(frac=0.01, random_state=1)\n",
    "        models_scalers = run_training(train_data,dataset=dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":    \n",
    "    \n",
    "    mode_name = \"src/HAmodel_scaler_cluster_0.0.pth\"\n",
    "    eval_df = evaluate_df(dataset, eval_mode)\n",
    "    eval_df = eval_df.head(40)\n",
    "    delta_velocity_pred = predict_delta_velocity(eval_df, mode_name)\n",
    "    print(f\"Predicted result:\", delta_velocity_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {cuda_available}\")\n",
    "\n",
    "# If CUDA is available, check the CUDA version and cuDNN availability\n",
    "if cuda_available:\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    cudnn_available = torch.backends.cudnn.is_available()\n",
    "    print(f\"cuDNN available: {cudnn_available}\")\n",
    "\n",
    "    # Print the version of cuDNN\n",
    "    if cudnn_available:\n",
    "        print(f\"cuDNN version: {torch.backends.cudnn.version()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Please ensure that you have a CUDA-capable GPU and the necessary CUDA drivers installed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # 拉取最新的变更\n",
    "# # !git pull origin master\n",
    "# # !git commit -m \"将 master 合并到 Tom\"\n",
    "\n",
    "# # 查看当前状态\n",
    "# !git status\n",
    "\n",
    "# # 添加所有变更的文件到暂存区\n",
    "# !git add .\n",
    "# # 提交变更，确保替换为你自己的提交信息\n",
    "# !git commit -m \"Update notebook with git commit example\"\n",
    "\n",
    "# # 推送到远程仓库的master分支\n",
    "# !git push origin Tom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "car-following-project--ZycwsGt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
