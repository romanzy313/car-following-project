{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Define the Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "# Define the Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        outputs, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "        predictions = self.linear(outputs)\n",
    "        return predictions, hidden, cell\n",
    "\n",
    "\n",
    "# Define the Seq2Seq model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_steps_out):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder(input_size, hidden_size)\n",
    "        self.decoder = Decoder(hidden_size, output_size)\n",
    "        self.n_steps_out = n_steps_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden, cell = self.encoder(x)\n",
    "        decoder_input = torch.zeros(\n",
    "            (x.size(0), self.n_steps_out, self.decoder.lstm.input_size)\n",
    "        ).to(x.device)\n",
    "        outputs, _, _ = self.decoder(decoder_input, hidden, cell)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuntimeModel:\n",
    "    def __init__(self, name: str):\n",
    "        # Load the checkpoint\n",
    "        self.checkpoint = torch.load(name)\n",
    "        # Extract the scaler from the checkpoint\n",
    "        self.scaler = self.checkpoint[\"scaler\"]\n",
    "\n",
    "    def preprocess_data_for_inference(self,df, n_steps_in=30, n_steps_out=10):\n",
    "        # Normalize the data\n",
    "        data_normalized = self.scaler.transform(df)  # Note: use transform, not fit_transform\n",
    "        # print(data_normalized.shape)\n",
    "        X = self.create_sequences_for_inference(data_normalized, n_steps_in, n_steps_out)\n",
    "        # print(X.shape)\n",
    "        return torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "    def create_sequences_for_inference(self, data, n_steps_in, n_steps_out):\n",
    "        X = []\n",
    "        for i in range(0, len(data) - n_steps_in - n_steps_out + 5):\n",
    "            seq_x = data[i : i + n_steps_in]\n",
    "            if seq_x.shape[0] == n_steps_in:\n",
    "                X.append(seq_x)\n",
    "        return np.array(X)\n",
    "\n",
    "    # this function needs to do all the work instead of `create_sequences_for_inference`\n",
    "    def process_runtime_data(self, df):\n",
    "        data_normalized = self.scaler.transform(df)\n",
    "        X = np.array(data_normalized)\n",
    "        return torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    def predict_delta_velocity(self, eval_df, delta_velocity_index=1):\n",
    "        \"\"\"\n",
    "        Safety-Critical Applications: If the prediction is used for real-time safety systems\n",
    "        (like advanced driver-assistance systems, ADAS), the most recent predictions may be the most\n",
    "        valuable as they can inform immediate safety interventions.\n",
    "\n",
    "        Driver Profiling or Long-Term Trends: If the goal is to understand long-term driver behavior\n",
    "        for insurance purposes or driver coaching, then averaging or aggregating over a range of predictions\n",
    "        to get a more stable and generalized profile might be more appropriate.\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"X_input_tensor\", eval_df.shape)\n",
    "        # Preprocess the data\n",
    "        X_new_tensor = self.preprocess_data_for_inference(eval_df, 10, 0)\n",
    "        # X_new_tensor = process_runtime_data(eval_df) # Need to use something like this instead\n",
    "        print(f\"X_new_tensor\", X_new_tensor.shape)\n",
    "        # Initialize the model based on the shape of the input data\n",
    "        model = Seq2Seq(\n",
    "            input_size=X_new_tensor.shape[2],\n",
    "            hidden_size=50,\n",
    "            n_steps_out=10,\n",
    "            output_size=X_new_tensor.shape[2],\n",
    "        )\n",
    "\n",
    "        # Load model's state dictionary from the checkpoint\n",
    "        model.load_state_dict(self.checkpoint[\"model_state_dict\"])\n",
    "\n",
    "        # Predict using the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_new_pred_tensor = model(X_new_tensor)\n",
    "            y_new_pred = y_new_pred_tensor.numpy()\n",
    "        # print(f\"y_new_pred_tensor\",y_test_tensor.shape)\n",
    "        print(y_new_pred.shape)\n",
    "\n",
    "        # Take the first prediction from the first sequence\n",
    "        y_first_pred = y_new_pred[0, :, :]\n",
    "        # print(f\"first predict\", y_first_pred.shape)\n",
    "        # print(y_first_pred)\n",
    "        # Inverse transform the predictions to the original scale\n",
    "\n",
    "        y_first_pred_original = self.scaler.inverse_transform(y_first_pred)\n",
    "\n",
    "        # Extract the denormalized delta_velocity values\n",
    "        delta_velocity_pred_original = y_first_pred_original[:, delta_velocity_index]\n",
    "\n",
    "        # Return the predicted delta velocity\n",
    "        return delta_velocity_pred_original\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_input_tensor (10, 3)\n",
      "X_new_tensor torch.Size([1, 10, 3])\n",
      "(1, 10, 3)\n",
      "Predicted result: [0.38137323 0.30774415 0.14285707 0.15704298 0.19867569 0.18528081\n",
      " 0.1643379  0.20671627 0.2630582  0.2929146 ]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# sample data\n",
    "\n",
    "model = RuntimeModel(\"model_scaler_cluster_1.pth\")\n",
    "\n",
    "runtime_data = pd.DataFrame(\n",
    "    {\n",
    "        \"delta_position\": [\n",
    "            25.147708,\n",
    "            24.986054,\n",
    "            24.847105,\n",
    "            24.691872,\n",
    "            24.533773,\n",
    "            24.374004,\n",
    "            24.209939,\n",
    "            24.042500,\n",
    "            23.879786,\n",
    "            23.715263,\n",
    "        ],\n",
    "        \"delta_velocity\": [\n",
    "            1.614755,\n",
    "            1.589909,\n",
    "            1.589107,\n",
    "            1.587685,\n",
    "            1.591374,\n",
    "            1.592214,\n",
    "            1.621730,\n",
    "            1.656232,\n",
    "            1.668315,\n",
    "            1.687577,\n",
    "        ],\n",
    "        \"v_follower\": [\n",
    "            11.265286,\n",
    "            11.227821,\n",
    "            11.216660,\n",
    "            11.199787,\n",
    "            11.174766,\n",
    "            11.139135,\n",
    "            11.118996,\n",
    "            11.104114,\n",
    "            11.075143,\n",
    "            11.052290,\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Sample usage\n",
    "delta_velocity_pred = model.predict_delta_velocity(\n",
    "    runtime_data, 1\n",
    ")\n",
    "print(f\"Predicted result:\", delta_velocity_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "car-following-project-UTHu64Qq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
