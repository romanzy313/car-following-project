{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from read_data import read_data\n",
    "\n",
    "def compute_delta_metrics(data):\n",
    "    \"\"\"\n",
    "    Computes additional metrics for the dataset:\n",
    "    - Delta Position: Leader's position minus Follower's position.\n",
    "    - Delta Velocity: Leader's velocity minus Follower's velocity.\n",
    "    - Delta Acceleration: Leader's acceleration minus Follower's acceleration.\n",
    "    - Time-To-Collision (TTC): Delta Position divided by Delta Velocity.\n",
    "    \"\"\"\n",
    "    data[\"delta_position\"] = data[\"x_leader\"] - data[\"x_follower\"]\n",
    "    data[\"delta_velocity\"] = data[\"v_leader\"] - data[\"v_follower\"]\n",
    "    data[\"delta_acceleration\"] = data[\"a_leader\"] - data[\"a_follower\"]\n",
    "    data[\"TTC\"] = data[\"delta_position\"] / data[\"delta_velocity\"]\n",
    "    return data\n",
    "\n",
    "def aggregate_data_by_case(data):\n",
    "    \"\"\"\n",
    "    Aggregates the dataset by 'case_id' to find the max and min \n",
    "    of each delta metric and TTC for each case.\n",
    "    Renames columns for clarity and adds case_id as a column.\n",
    "    \"\"\"\n",
    "    aggr_data = data.groupby('case_id').agg({\n",
    "        'delta_position': ['max', 'min'], \n",
    "        'delta_velocity': ['max', 'min'], \n",
    "        'delta_acceleration': ['max', 'min'], \n",
    "        'TTC': ['max', 'min']\n",
    "    })\n",
    "    aggr_data.columns = [\"_\".join(x) for x in aggr_data.columns.ravel()]\n",
    "    aggr_data[\"case_id\"] = aggr_data.index\n",
    "    aggr_data.columns = [\n",
    "        'max_delta_position', 'min_delta_position', \n",
    "        'max_delta_velocity', 'min_delta_velocity', \n",
    "        'max_delta_acceleration', 'min_delta_acceleration', \n",
    "        'max_TTC', 'min_TTC', 'case_id'\n",
    "    ]\n",
    "    aggr_data = aggr_data[\n",
    "        ['case_id', 'max_delta_position', 'min_delta_position', \n",
    "        'max_delta_velocity', 'min_delta_velocity', \n",
    "        'max_delta_acceleration', 'min_delta_acceleration', \n",
    "        'max_TTC', 'min_TTC']\n",
    "    ]\n",
    "    return aggr_data\n",
    "\n",
    "def adjust_ttc_sign(aggregated_data):\n",
    "    \"\"\"\n",
    "    Ensures TTC (Time-To-Collision) is non-negative by taking the absolute value.\n",
    "    \"\"\"\n",
    "    aggregated_data[\"min_TTC\"] = aggregated_data[\"min_TTC\"].abs()\n",
    "    return aggregated_data\n",
    "\n",
    "def convert_df(dataset: str, mode: str):\n",
    "    \"\"\"\n",
    "    Main function that utilizes the above helper functions to preprocess the data.\n",
    "    Returns a DataFrame grouped by 'case_id' with max and min values of\n",
    "    delta position, delta velocity, delta acceleration, and TTC (Time-To-Collision).\n",
    "    \"\"\"\n",
    "    data = read_data(dataset, mode)\n",
    "    data = compute_delta_metrics(data)\n",
    "    aggregated_data = aggregate_data_by_case(data)\n",
    "    aggregated_data = adjust_ttc_sign(aggregated_data)\n",
    "    return aggregated_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>max_delta_position</th>\n",
       "      <th>min_delta_position</th>\n",
       "      <th>max_delta_velocity</th>\n",
       "      <th>min_delta_velocity</th>\n",
       "      <th>max_delta_acceleration</th>\n",
       "      <th>min_delta_acceleration</th>\n",
       "      <th>max_TTC</th>\n",
       "      <th>min_TTC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>26394.000000</td>\n",
       "      <td>26394.000000</td>\n",
       "      <td>26394.000000</td>\n",
       "      <td>26394.000000</td>\n",
       "      <td>26394.000000</td>\n",
       "      <td>26394.000000</td>\n",
       "      <td>26394.000000</td>\n",
       "      <td>2.639400e+04</td>\n",
       "      <td>2.639400e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>13196.500000</td>\n",
       "      <td>25.333803</td>\n",
       "      <td>12.660995</td>\n",
       "      <td>1.907915</td>\n",
       "      <td>-1.961766</td>\n",
       "      <td>1.214429</td>\n",
       "      <td>-1.377007</td>\n",
       "      <td>1.946642e+05</td>\n",
       "      <td>2.023333e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7619.435839</td>\n",
       "      <td>11.389465</td>\n",
       "      <td>7.005050</td>\n",
       "      <td>1.246547</td>\n",
       "      <td>1.262682</td>\n",
       "      <td>0.625144</td>\n",
       "      <td>1.297498</td>\n",
       "      <td>1.134597e+07</td>\n",
       "      <td>1.763010e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.970585</td>\n",
       "      <td>3.192059</td>\n",
       "      <td>-2.645687</td>\n",
       "      <td>-15.422088</td>\n",
       "      <td>-0.164731</td>\n",
       "      <td>-10.560531</td>\n",
       "      <td>-5.370677e+01</td>\n",
       "      <td>2.865257e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6598.250000</td>\n",
       "      <td>18.117176</td>\n",
       "      <td>7.330479</td>\n",
       "      <td>0.874794</td>\n",
       "      <td>-2.585847</td>\n",
       "      <td>0.716436</td>\n",
       "      <td>-1.457096</td>\n",
       "      <td>1.228487e+03</td>\n",
       "      <td>1.226827e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13196.500000</td>\n",
       "      <td>22.091704</td>\n",
       "      <td>11.508556</td>\n",
       "      <td>1.803440</td>\n",
       "      <td>-1.798556</td>\n",
       "      <td>1.164713</td>\n",
       "      <td>-1.044787</td>\n",
       "      <td>3.797345e+03</td>\n",
       "      <td>3.719639e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>19794.750000</td>\n",
       "      <td>28.647320</td>\n",
       "      <td>15.768843</td>\n",
       "      <td>2.779035</td>\n",
       "      <td>-1.054550</td>\n",
       "      <td>1.601606</td>\n",
       "      <td>-0.748433</td>\n",
       "      <td>1.528928e+04</td>\n",
       "      <td>1.520460e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>26393.000000</td>\n",
       "      <td>85.211403</td>\n",
       "      <td>78.518199</td>\n",
       "      <td>13.742090</td>\n",
       "      <td>1.298914</td>\n",
       "      <td>6.417317</td>\n",
       "      <td>-0.032989</td>\n",
       "      <td>1.579993e+09</td>\n",
       "      <td>2.840646e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            case_id  max_delta_position  min_delta_position  \\\n",
       "count  26394.000000        26394.000000        26394.000000   \n",
       "mean   13196.500000           25.333803           12.660995   \n",
       "std     7619.435839           11.389465            7.005050   \n",
       "min        0.000000            7.970585            3.192059   \n",
       "25%     6598.250000           18.117176            7.330479   \n",
       "50%    13196.500000           22.091704           11.508556   \n",
       "75%    19794.750000           28.647320           15.768843   \n",
       "max    26393.000000           85.211403           78.518199   \n",
       "\n",
       "       max_delta_velocity  min_delta_velocity  max_delta_acceleration  \\\n",
       "count        26394.000000        26394.000000            26394.000000   \n",
       "mean             1.907915           -1.961766                1.214429   \n",
       "std              1.246547            1.262682                0.625144   \n",
       "min             -2.645687          -15.422088               -0.164731   \n",
       "25%              0.874794           -2.585847                0.716436   \n",
       "50%              1.803440           -1.798556                1.164713   \n",
       "75%              2.779035           -1.054550                1.601606   \n",
       "max             13.742090            1.298914                6.417317   \n",
       "\n",
       "       min_delta_acceleration       max_TTC       min_TTC  \n",
       "count            26394.000000  2.639400e+04  2.639400e+04  \n",
       "mean                -1.377007  1.946642e+05  2.023333e+05  \n",
       "std                  1.297498  1.134597e+07  1.763010e+07  \n",
       "min                -10.560531 -5.370677e+01  2.865257e+00  \n",
       "25%                 -1.457096  1.228487e+03  1.226827e+03  \n",
       "50%                 -1.044787  3.797345e+03  3.719639e+03  \n",
       "75%                 -0.748433  1.528928e+04  1.520460e+04  \n",
       "max                 -0.032989  1.579993e+09  2.840646e+09  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainHA = convert_df(\"HA\", \"train\")\n",
    "trainHA.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = trainHA.head(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:00<00:04, 22.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 0.25866872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [00:00<00:02, 32.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 loss: 0.19594710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [00:00<00:02, 30.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 loss: 0.13900389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [00:01<00:02, 28.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30 loss: 0.08113288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47/100 [00:01<00:01, 31.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40 loss: 0.03783986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [00:01<00:01, 31.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50 loss: 0.02038259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 67/100 [00:02<00:01, 32.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 60 loss: 0.01740770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75/100 [00:02<00:00, 33.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 70 loss: 0.01689392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [00:02<00:00, 33.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 80 loss: 0.01606349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95/100 [00:03<00:00, 31.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 90 loss: 0.01554240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 30.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 148191870976.00\n",
      "Root Mean Squared Error (RMSE): 384956.97\n",
      "Mean Absolute Error (MAE): 41951.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(148191870000.0, 384956.9728891789, 41951.61)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=100, output_size=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "    \n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, _ = self.lstm(input_seq)\n",
    "        predictions = self.linear(lstm_out[:, -1, :])\n",
    "        return predictions\n",
    "\n",
    "def create_sequences(data, n_steps_in, n_steps_out):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data)):\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        if out_end_ix > len(data):\n",
    "            break\n",
    "        X.append(data[i:end_ix, :])\n",
    "        y.append(data[end_ix:out_end_ix, :])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def preprocess_data(df, n_steps_in, n_steps_out, test_size=0.2):\n",
    "    scaler = MinMaxScaler()\n",
    "    data_normalized = scaler.fit_transform(df.values)\n",
    "    X, y = create_sequences(data_normalized, n_steps_in, n_steps_out)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    return (\n",
    "        torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32),\n",
    "        torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32),\n",
    "        scaler\n",
    "    )\n",
    "\n",
    "def train_model(model, X_train_tensor, y_train_tensor, epochs, optimizer, loss_function):\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_train_tensor)\n",
    "        loss = loss_function(y_pred, y_train_tensor[:,0,:])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch%10 == 0:\n",
    "            tqdm.write(f'epoch: {epoch} loss: {loss.item():.8f}')\n",
    "\n",
    "def evaluate_model(model, X_test_tensor, y_test_tensor, scaler):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_test_pred = model(X_test_tensor)\n",
    "        y_test_pred_np = scaler.inverse_transform(y_test_pred.numpy())\n",
    "        y_test_actual_np = scaler.inverse_transform(y_test_tensor[:, 0, :].numpy())\n",
    "        \n",
    "        mse = mean_squared_error(y_test_actual_np, y_test_pred_np)\n",
    "        rmse = math.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test_actual_np, y_test_pred_np)\n",
    "    \n",
    "    print(f'Mean Squared Error (MSE): {mse:.2f}')\n",
    "    print(f'Root Mean Squared Error (RMSE): {rmse:.2f}')\n",
    "    print(f'Mean Absolute Error (MAE): {mae:.2f}')\n",
    "    \n",
    "    return mse, rmse, mae\n",
    "\n",
    "# Usage\n",
    "# Assume df is your DataFrame\n",
    "n_steps_in, n_steps_out = 3, 1\n",
    "X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, scaler = preprocess_data(df, n_steps_in, n_steps_out)\n",
    "\n",
    "model = LSTMModel(input_size=X_train_tensor.shape[2], hidden_layer_size=50, output_size=X_train_tensor.shape[2])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "train_model(model, X_train_tensor, y_train_tensor, epochs=100, optimizer=optimizer, loss_function=loss_function)\n",
    "evaluate_model(model, X_test_tensor, y_test_tensor, scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = convert_df(\"HA\", \"val\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (lstm): LSTM(9, 50, batch_first=True)\n",
       "  (linear): Linear(in_features=50, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "car-following-project--ZycwsGt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
