{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from read_data import read_data\n",
    "\n",
    "original_data_path = './data/' # this is my path, change it to yours\n",
    "# separated_data_path = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read data\n",
    "# def read_data(setname):\n",
    "#     data = pd.read_hdf(original_data_path+\"train\"+setname+'.zarr', key='data') # fill in your path and file name\n",
    "#     return data[['case_id','time','x_leader','x_follower','v_leader','v_follower']]\n",
    "from read_data import read_data\n",
    "\n",
    "\n",
    "def read_data_wrapper(setname, type):\n",
    "    # data = pd.read_hdf(\n",
    "    #     f\"{original_data_path}/{type}{setname}.zarr\"\n",
    "    # )  # fill in your path and file name\n",
    "    data = read_data(setname, type)\n",
    "    return data[[\"case_id\", \"time\", \"x_leader\", \"x_follower\", \"v_leader\", \"v_follower\"]]\n",
    "# data_HA = read_data('HA') # I guess this is HA_train or something in your PC\n",
    "# data_HH = read_data('HH')\n",
    "\n",
    "data_HA = read_data_wrapper(\"HA\", \"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>time</th>\n",
       "      <th>x_leader</th>\n",
       "      <th>x_follower</th>\n",
       "      <th>v_leader</th>\n",
       "      <th>v_follower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-9.428864</td>\n",
       "      <td>2.629775</td>\n",
       "      <td>0.784441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.263154</td>\n",
       "      <td>-9.341833</td>\n",
       "      <td>2.665530</td>\n",
       "      <td>0.810204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.535099</td>\n",
       "      <td>-9.265412</td>\n",
       "      <td>2.776808</td>\n",
       "      <td>0.828016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.828920</td>\n",
       "      <td>-9.107064</td>\n",
       "      <td>2.966034</td>\n",
       "      <td>0.889182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.147062</td>\n",
       "      <td>-8.933068</td>\n",
       "      <td>3.168645</td>\n",
       "      <td>0.979196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   case_id  time  x_leader  x_follower  v_leader  v_follower\n",
       "0        0   0.0  0.000000   -9.428864  2.629775    0.784441\n",
       "1        0   0.1  0.263154   -9.341833  2.665530    0.810204\n",
       "2        0   0.2  0.535099   -9.265412  2.776808    0.828016\n",
       "3        0   0.3  0.828920   -9.107064  2.966034    0.889182\n",
       "4        0   0.4  1.147062   -8.933068  3.168645    0.979196"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_HA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26394/26394 [01:17<00:00, 339.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples in HA: 159369\n"
     ]
    }
   ],
   "source": [
    "# Segment data to make 30 timesteps input and 10 timesteps output\n",
    "def segment_data(data):\n",
    "    data = data.copy()\n",
    "    data['delta_velocity'] = data['v_follower'] - data['v_leader']\n",
    "    data['delta_position'] = data['x_leader'] - data['x_follower']\n",
    "    data = data.sort_values(by=['case_id','time']).set_index('case_id')\n",
    "    features = []\n",
    "    labels = []\n",
    "    idx = 0\n",
    "    for case_id in tqdm(data.index.unique()):\n",
    "        df = data.loc[case_id]\n",
    "        future_idx_end = np.arange(40,len(df),40) # This line creates samples without overlapping, do that if the data amount is not enough or as you wish\n",
    "        # future_idx_end = np.concatenate((future_idx_end, future_idx_end[1:]-15)) # make 10 timesteps overlapping, of course running time will also double\n",
    "        future_idx_start = future_idx_end - 10\n",
    "        history_idx_end = future_idx_start\n",
    "        history_idx_start = history_idx_end - 30\n",
    "        for hstart,hend,fstart,fend in zip(history_idx_start,history_idx_end,future_idx_start,future_idx_end):\n",
    "            feature = df.iloc[hstart:hend][['time','delta_velocity','delta_position','v_follower']].copy()\n",
    "            feature['sample_id'] = idx\n",
    "            label = df.iloc[fstart:fend][['time','v_follower']].copy()\n",
    "            label['sample_id'] = idx\n",
    "            features.append(feature)\n",
    "            labels.append(label)\n",
    "            idx += 1\n",
    "    features = pd.concat(features).reset_index()\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    features[['delta_velocity','delta_position','v_follower']] = scaler.fit_transform(features[['delta_velocity','delta_position','v_follower']])\n",
    "    # But do not standardize labels\n",
    "    labels = pd.concat(labels).reset_index()\n",
    "    return features, labels\n",
    "\n",
    "features_HA, labels_HA = segment_data(data_HA.loc[data_HA['case_id']<(1e5+500)])\n",
    "print('number of samples in HA:', labels_HA['sample_id'].nunique())\n",
    "# features_HA.to_hdf(separated_data_path+'features_HA.h5', key='features') # or save in other format you are familiar with\n",
    "# labels_HA.to_hdf(separated_data_path+'labels_HA.h5', key='labels')\n",
    "# features_HH, labels_HH = segment_data(data_HH)\n",
    "# print('number of samples in HH:', labels_HH['sample_id'].nunique())\n",
    "# features_HH.to_hdf(separated_data_path+'features_HH.h5', key='features')\n",
    "# labels_HH.to_hdf(separated_data_path+'labels_HH.h5', key='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read features and labels in local\n",
    "features_HA = pd.read_hdf(separated_data_path+'features_HA.h5', key='features')\n",
    "labels_HA = pd.read_hdf(separated_data_path+'labels_HA.h5', key='labels')\n",
    "features_HH = pd.read_hdf(separated_data_path+'features_HH.h5', key='features')\n",
    "labels_HH = pd.read_hdf(separated_data_path+'labels_HH.h5', key='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(features,labels):\n",
    "    # Split data into training, validation, test set by idx\n",
    "    # make sure the random choice of features and labels are the same!\n",
    "    all_indices_HA = labels['sample_id'].unique()\n",
    "    train_indices_HA = np.random.choice(all_indices_HA, size=int(0.7*len(all_indices_HA)), replace=False)\n",
    "    test_indices_HA = np.random.choice(np.setdiff1d(all_indices_HA,train_indices_HA), size=int(0.3*len(all_indices_HA)), replace=False)\n",
    "    # val_set you can apply the previous code to val_HA that is already existing\n",
    "    train_features_HA = features[features['sample_id'].isin(train_indices_HA)]\n",
    "    train_labels_HA = labels[labels['sample_id'].isin(train_indices_HA)]\n",
    "    test_features_HA = features[features['sample_id'].isin(test_indices_HA)]\n",
    "    test_labels_HA = labels[labels['sample_id'].isin(test_indices_HA)]\n",
    "\n",
    "    return train_features_HA, train_labels_HA, test_features_HA, test_labels_HA\n",
    "\n",
    "train_features_HA, train_labels_HA, test_features_HA, test_labels_HA = data_split(features_HA,labels_HA)\n",
    "# the same for HH\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader function\n",
    "class CreateDataset:\n",
    "    def __init__(self, features, labels):\n",
    "        self.idx_list = labels['sample_id'].unique()\n",
    "        self.labels = labels.sort_values(['sample_id','time']).set_index('sample_id')\n",
    "        self.features = features.sort_values(['sample_id','time']).set_index('sample_id')        \n",
    "\n",
    "    def __len__(self):        \n",
    "        return len(self.idx_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # idx is the index of items in the data and labels\n",
    "        sample_id = self.idx_list[idx]\n",
    "        history = self.features.loc[sample_id][['delta_velocity','delta_position','v_follower']].values\n",
    "        history = torch.from_numpy(history).float()\n",
    "        future = self.labels.loc[sample_id]['v_follower'].values\n",
    "        future = torch.from_numpy(future).float()\n",
    "        return history, future\n",
    "    \n",
    "# Create dataloader to be used\n",
    "train_dataloader_HA = DataLoader(CreateDataset(train_features_HA, train_labels_HA), batch_size=64, shuffle=False) # batch_size can also be 128\n",
    "# ... the same for others\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 30, 3])\n",
      "Labels batch shape: torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# Test if the dataloader works\n",
    "history, future = next(iter(train_dataloader_HA))\n",
    "print(f\"Feature batch shape: {history.size()}\")\n",
    "print(f\"Labels batch shape: {future.size()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5460, -0.8946, -1.7200],\n",
       "         [-1.5539, -0.8764, -1.7146],\n",
       "         [-1.6281, -0.8562, -1.7108],\n",
       "         ...,\n",
       "         [-1.2533, -0.5123, -0.7601],\n",
       "         [-1.0633, -0.5150, -0.6872],\n",
       "         [-0.8279, -0.5253, -0.6057]],\n",
       "\n",
       "        [[-0.4961, -0.5692, -0.3384],\n",
       "         [-0.6173, -0.5619, -0.3544],\n",
       "         [-0.7183, -0.5502, -0.3641],\n",
       "         ...,\n",
       "         [-1.8279, -0.3294, -0.2099],\n",
       "         [-1.8093, -0.3104, -0.1864],\n",
       "         [-1.7922, -0.2925, -0.1580]],\n",
       "\n",
       "        [[ 0.2521, -0.2318,  0.5332],\n",
       "         [ 0.2888, -0.2482,  0.5483],\n",
       "         [ 0.2394, -0.2512,  0.5467],\n",
       "         ...,\n",
       "         [ 0.4438, -0.2793,  0.5700],\n",
       "         [ 0.5182, -0.3038,  0.5849],\n",
       "         [ 0.5689, -0.3327,  0.5922]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.1149, -1.1914, -1.8954],\n",
       "         [-0.1117, -1.1853, -1.8945],\n",
       "         [-0.0882, -1.1851, -1.8883],\n",
       "         ...,\n",
       "         [-0.0885, -1.1987, -1.8883],\n",
       "         [-0.0877, -1.1996, -1.8881],\n",
       "         [-0.1005, -1.1951, -1.8915]],\n",
       "\n",
       "        [[-0.0901, -1.1855, -1.8887],\n",
       "         [-0.0779, -1.1886, -1.8855],\n",
       "         [-0.0833, -1.1866, -1.8869],\n",
       "         ...,\n",
       "         [-1.2177, -1.1224, -1.8764],\n",
       "         [-1.3876, -1.1044, -1.8730],\n",
       "         [-1.5295, -1.0852, -1.8669]],\n",
       "\n",
       "        [[-2.3542, -0.8236, -1.5976],\n",
       "         [-2.2647, -0.8083, -1.5385],\n",
       "         [-2.1955, -0.7919, -1.4798],\n",
       "         ...,\n",
       "         [-1.3716, -0.5372, -0.4155],\n",
       "         [-1.3978, -0.5235, -0.3959],\n",
       "         [-1.3952, -0.5083, -0.3762]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
