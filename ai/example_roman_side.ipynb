{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "original_data_path = './CFdata/' # this is my path, change it to yours\n",
    "separated_data_path = '../super_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_data import read_data\n",
    "\n",
    "\n",
    "def read_data_wrapper(setname, type):\n",
    "    # data = pd.read_hdf(\n",
    "    #     f\"{original_data_path}/{type}{setname}.zarr\"\n",
    "    # )  # fill in your path and file name\n",
    "    data = read_data(setname, type)\n",
    "    return data[[\"case_id\", \"time\", \"x_leader\", \"x_follower\", \"v_leader\", \"v_follower\"]]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26394/26394 [01:43<00:00, 254.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples in HA: 159369\n"
     ]
    }
   ],
   "source": [
    "# Segment data to make 30 timesteps input and 10 timesteps output\n",
    "def segment_data(data):\n",
    "    data = data.copy()\n",
    "    data['delta_velocity'] = data['v_follower'] - data['v_leader']\n",
    "    data['delta_position'] = data['x_leader'] - data['x_follower']\n",
    "    data = data.sort_values(by=['case_id','time']).set_index('case_id')\n",
    "    features = []\n",
    "    labels = []\n",
    "    idx = 0\n",
    "    for case_id in tqdm(data.index.unique()):\n",
    "        df = data.loc[case_id]\n",
    "        future_idx_end = np.arange(40,len(df),40) # This line creates samples without overlapping, do that if the data amount is not enough or as you wish\n",
    "        # future_idx_end = np.concatenate((future_idx_end, future_idx_end[1:]-15)) # make 10 timesteps overlapping, of course running time will also double\n",
    "        future_idx_start = future_idx_end - 10\n",
    "        history_idx_end = future_idx_start\n",
    "        history_idx_start = history_idx_end - 30\n",
    "        for hstart,hend,fstart,fend in zip(history_idx_start,history_idx_end,future_idx_start,future_idx_end):\n",
    "            feature = df.iloc[hstart:hend][['time','delta_velocity','delta_position','v_follower']].copy()\n",
    "            feature['sample_id'] = idx\n",
    "            label = df.iloc[fstart:fend][['time','v_follower']].copy()\n",
    "            label['sample_id'] = idx\n",
    "            features.append(feature)\n",
    "            labels.append(label)\n",
    "            idx += 1\n",
    "    features = pd.concat(features).reset_index()\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    features[['delta_velocity','delta_position','v_follower']] = scaler.fit_transform(features[['delta_velocity','delta_position','v_follower']])\n",
    "    # But do not standardize labels\n",
    "    labels = pd.concat(labels).reset_index()\n",
    "    return features, labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26394/26394 [01:45<00:00, 251.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples in HA: 159369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38683/38683 [01:49<00:00, 351.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples in HH: 166365\n"
     ]
    }
   ],
   "source": [
    "## HA\n",
    "\n",
    "data_HA = read_data_wrapper(\n",
    "    \"HA\", \"train\"\n",
    ")  # I guess this is HA_train or something in your PC\n",
    "features_HA, labels_HA = segment_data(data_HA.loc[data_HA[\"case_id\"] < (1e5 + 500)])\n",
    "print(\"number of samples in HA:\", labels_HA[\"sample_id\"].nunique())\n",
    "features_HA.to_hdf(\n",
    "    separated_data_path + \"features_HA.h5\", key=\"features\"\n",
    ")  # or save in other format you are familiar with\n",
    "\n",
    "## HH\n",
    "\n",
    "data_HH = read_data_wrapper(\"HH\", \"train\")\n",
    "labels_HA.to_hdf(separated_data_path + \"labels_HA.h5\", key=\"labels\")\n",
    "features_HH, labels_HH = segment_data(data_HH)\n",
    "print(\"number of samples in HH:\", labels_HH[\"sample_id\"].nunique())\n",
    "features_HH.to_hdf(separated_data_path + \"features_HH.h5\", key=\"features\")\n",
    "labels_HH.to_hdf(separated_data_path + \"labels_HH.h5\", key=\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File ../separated/features_HA.h5 does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/var/www/car-following-project/ai/example.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/var/www/car-following-project/ai/example.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Read features and labels in local\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/var/www/car-following-project/ai/example.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m features_HA \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_hdf(separated_data_path\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mfeatures_HA.h5\u001b[39;49m\u001b[39m'\u001b[39;49m, key\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mfeatures\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/var/www/car-following-project/ai/example.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m labels_HA \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_hdf(separated_data_path\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlabels_HA.h5\u001b[39m\u001b[39m'\u001b[39m, key\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/var/www/car-following-project/ai/example.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m features_HH \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_hdf(separated_data_path\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfeatures_HH.h5\u001b[39m\u001b[39m'\u001b[39m, key\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/car-following-project-UTHu64Qq/lib/python3.11/site-packages/pandas/io/pytables.py:424\u001b[0m, in \u001b[0;36mread_hdf\u001b[0;34m(path_or_buf, key, mode, errors, where, start, stop, columns, iterator, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m     exists \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m exists:\n\u001b[0;32m--> 424\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFile \u001b[39m\u001b[39m{\u001b[39;00mpath_or_buf\u001b[39m}\u001b[39;00m\u001b[39m does not exist\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    426\u001b[0m store \u001b[39m=\u001b[39m HDFStore(path_or_buf, mode\u001b[39m=\u001b[39mmode, errors\u001b[39m=\u001b[39merrors, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    427\u001b[0m \u001b[39m# can't auto open/close if we are using an iterator\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[39m# so delegate to the iterator\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File ../separated/features_HA.h5 does not exist"
     ]
    }
   ],
   "source": [
    "# Read features and labels in local\n",
    "features_HA = pd.read_hdf(separated_data_path+'features_HA.h5', key='features')\n",
    "labels_HA = pd.read_hdf(separated_data_path+'labels_HA.h5', key='labels')\n",
    "features_HH = pd.read_hdf(separated_data_path+'features_HH.h5', key='features')\n",
    "labels_HH = pd.read_hdf(separated_data_path+'labels_HH.h5', key='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, test set by idx\n",
    "# make sure the random choice of features and labels are the same!\n",
    "all_indices_HA = labels_HA['sample_id'].unique()\n",
    "train_indices_HA = np.random.choice(all_indices_HA, size=int(0.7*len(all_indices_HA)), replace=False)\n",
    "test_indices_HA = np.random.choice(np.setdiff1d(all_indices_HA,train_indices_HA), size=int(0.3*len(all_indices_HA)), replace=False)\n",
    "# val_set you can apply the previous code to val_HA that is already existing\n",
    "train_features_HA = features_HA[features_HA['sample_id'].isin(train_indices_HA)]\n",
    "train_labels_HA = labels_HA[labels_HA['sample_id'].isin(train_indices_HA)]\n",
    "test_features_HA = features_HA[features_HA['sample_id'].isin(test_indices_HA)]\n",
    "test_labels_HA = labels_HA[labels_HA['sample_id'].isin(test_indices_HA)]\n",
    "# the same for HH\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader function\n",
    "class CreateDataset:\n",
    "    def __init__(self, features, labels):\n",
    "        self.idx_list = labels['sample_id'].unique()\n",
    "        self.labels = labels.sort_values(['sample_id','time']).set_index('sample_id')\n",
    "        self.features = features.sort_values(['sample_id','time']).set_index('sample_id')        \n",
    "\n",
    "    def __len__(self):        \n",
    "        return len(self.idx_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # idx is the index of items in the data and labels\n",
    "        sample_id = self.idx_list[idx]\n",
    "        history = self.features.loc[sample_id][['delta_velocity','delta_position','v_follower']].values\n",
    "        history = torch.from_numpy(history).float()\n",
    "        future = self.labels.loc[sample_id]['v_follower'].values\n",
    "        future = torch.from_numpy(future).float()\n",
    "        return history, future\n",
    "    \n",
    "# Create dataloader to be used\n",
    "train_dataloader_HA = DataLoader(CreateDataset(train_features_HA, train_labels_HA), batch_size=64, shuffle=False) # batch_size can also be 128\n",
    "# ... the same for others\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if the dataloader works\n",
    "history, future = next(iter(train_dataloader_HA))\n",
    "print(f\"Feature batch shape: {history.size()}\")\n",
    "print(f\"Labels batch shape: {future.size()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
